{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers are built on <strong>algorithms</strong> which are a set of instructions that would be used to solve a particular problem/calculation or complete a particular task. Combining algorithms into a program and implementing conditions that allow decisions to be made and loops to allow the processes to be repeated form the basis for AI (Artificial Intelligence). \n",
    "Algorithms can get very complex, but ultimately come down to “if X situation occurs, then do Y behaviour.\n",
    "    \n",
    "https://www.enginess.io/insights/machine-learning-everything-you-need-to-know\n",
    "\n",
    "\n",
    "AI or <strong>Aritifical Inteligence</strong> has made more repetitive tasks easier to complete on a large scale such as clerical work, invoicing, and management reporting. It is a technique that allows machines to simulate human behaviour and make \"intellegent\" decisions. <strong>Machine learning</strong> is a [subset](https://www.ibm.com/cloud/learn/what-is-artificial-intelligence) of AI and is used to identify patterns and predict outcomes and improve with experience and exposure to more data without being explicitly programmed.\n",
    "\n",
    "<center>\n",
    "<img src=\"Images for Scikit Learn Noebook/1_V-qg5pihTZZBPWa71jDg5g.png\" width=\"600\" height=\"450\" Title=\"Machine Learning\"/>\n",
    "</center>  \n",
    "\n",
    "<center>\n",
    "<img src=\"https://github.com/colmhiggs11/Machine_Learning_21_CH/blob/main/Images%20for%20Scikit%20Learn%20Noebook/1_V-qg5pihTZZBPWa71jDg5g.png?raw=true\" width=\"600\" height=\"450\" Title=\"Machine Learning\"/>\n",
    "</center>  \n",
    "\n",
    "![Fisher Data Table](https://github.com/colmhiggs11/Machine_Learning_21_CH/blob/main/Images%20for%20Scikit%20Learn%20Noebook/1_V-qg5pihTZZBPWa71jDg5g.png?raw=true)\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Machine Learning is the process of teaching machines to take those algorithms and improve on them by extracting knowledge from the data. As the machine iteracts with more data over time it will improve on the algorithm and be able to make predictions by finding patterns in the data.\n",
    "</p>\n",
    "\n",
    "\n",
    "https://www.datarevenue.com/en-blog/introduction-to-machine-learning-for-managers\n",
    "Predictions in Machine learning takes into account both \"What will happen in the future\" questions and predictions on future data (house prices etc) but also predictions on data that require classifications Eg. (What type of animal is this?, Does this person have glaucoma?) etc.\n",
    "\n",
    "---\n",
    "## Applications of Machine Learning\n",
    "---\n",
    "Some examples of the applications are shown in the image [Applications of Machine Learning](#colm). This list is obviously non exhaustive and is applications for machine learning are constantly growing in every industry. \n",
    "\n",
    "\n",
    ">A 2020 [Deloitte survey](https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/state-of-ai-and-intelligent-automation-in-business-survey.html) found that 67% of companies are using machine learning, and 97% are using or planning to use it in the next year.\n",
    "\n",
    "\n",
    "<center>\n",
    "<a name = \"colm\">\n",
    "<img src=\"https://github.com/colmhiggs11/Machine_Learning_21_CH/blob/main/Images%20for%20Scikit%20Learn%20Noebook/0_IrxP1sZ14Z0qqIic.png\" width=\"50%\" height=\"50%\" Title=\"Machine Learning\">\n",
    "</a>  \n",
    "</center>\n",
    "\n",
    "The robotic surgery mentioned in the [image](#colm) above is one that is of personal interest to myself. I recently got Laser eye surgery and the process of the surgery was very interesting. Your eye undergoes a 3-d mapping study to determine shape, eye health, curvature etc. The actual surgery then is completed by loading the data into the the robot and burning away layers of the cornea into the desired shape. It does all of this work while being able to predict movements of the eye. \n",
    "    \n",
    "Another example of this might be a self driving car. Initially you could program the car to stop if it detects an object within 5 metres. With machine learning and after being exposed to lots and lots of data the machine would be able to tell the difference and decide to stop if a human was infront of the car but realise it would not need to stop if leafs or a newspaper for example were to come within the range.\n",
    "\n",
    "Some other examples of machine learning that we would come across on a day to day basis include: \n",
    "    \n",
    "|Eamples of Machine Learning in everday life| Success of Machine Learning\n",
    "|:-------------|:-------------|\n",
    "|Prediciting the amount of inventory that should be ordered in shops |he huge quantities of data available in a variety of different formats.|\n",
    "|Netflix recommending what movies to watch   |The technology advances and the processing power that is available today|\n",
    "|What songs you will like based on your current playlists |\n",
    "|Recognising people you know in your photos bases on the amount of times the same face appears |\n",
    "|Deciding whether a candidate would be suitable to hire based on analysis of their CV  |\n",
    "|What songs you will like based on your current playlists |   \n",
    "\n",
    "\n",
    "|     Stage    |   Description      |\n",
    "|--------------|-----------------|\n",
    "| __Load Data__ | You need to load the data in sklearn to get the machine learninf nodel to work|\n",
    "\n",
    "\n",
    "   \n",
    "` The most successful kinds of machine learning algorithms are those that automate decision-making processes by generalizing from known examples. In this setting, which is known as supervised learning, the user provides the algorithm with pairs of inputs and desired outputs, and the algorithm finds a way to produce the desired out‐ put given an input. In particular, the algorithm is able to create an output for an input it has never seen before without any help from a human.\n",
    "Any information or data sent to a computer is considered input.\n",
    "Data produced by a computer is considered output. \n",
    "In the machine learning community, input data is referred to as the feature set and output data is referred to as the target. \n",
    "Sample data is typically referred to as training data. Once the algorithm is trained with sample data, it can make predictions on new data. \n",
    "New data is typically referred to as test data. \n",
    "If your application can be formulated as a supervised learning problem, and you are able to create a dataset that includes the desired outcome, machine learning will likely be able to solve your problem. `\n",
    "\n",
    "Three main types of Machine Learning are Supervised, Unsupervise and Reinforcement learning. For the purpose of this work the focus will be on Supervised and Unsupervised learning.\n",
    "\n",
    "**Decision tree for type of Machine Learning** | **Uses for various types of Machine Learning**\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5efef46097a154395f56f43d_image6_s.png\" width=\"75%\" height=\"75%\" Title=\"Machine Learning\"> | <img src=\"../Images for Scikit Learn Noebook/1_8wU0hfUY3UK_D8Y7tbIyFQ (1).png\" width=\"85%\" height=\"85%\" Title=\"Machine Learning\"> \n",
    "\n",
    "\n",
    "\n",
    "## Supervised & Unsupervised Learning\n",
    "\n",
    ">Oreily book>\n",
    "\n",
    "### Supervised\n",
    "---\n",
    "The most successful kinds of machine learning algorithms are those that manipulate input data and predict the outputs. Also updating the outputs as it interacts with new data.\n",
    "The data which in this case is the **INPUT** is passed into the algorithm that is provided/chosen by the user. The user trains the data by pairing the inputs with the desired outputs and the algorithm will then create an **OUTPUT** for any new given input. In Machine Learning the input dataset is known as the **FEATURE SET** and the output dataset is known as the **TARGET**. The Sample input data used to train the algorithm is called the **TRAINING DATA** and the new data is referred to as the **TEST DATA**. Examples of what Supervised learning is typically used for can be seen in the [image](#colm1) above.  \n",
    "\n",
    "Classification - organisation of labelled data\n",
    "\n",
    "Regression - prediction of trends in labelled data to determine future outcomes\n",
    "    \n",
    "    \n",
    "### Unsupervised\n",
    "---\n",
    "\n",
    "In Unsupervised learning only the input values or training data is not labelled and no output is given to the algorithm to train it with which can make it more difficult to understand. In unsupervised learning there are no correct answers. \n",
    "This means the accuracy can be difficult to measure with unsupervised learning. It creates a less controllable environment as the machine is creating the outputs \n",
    "\n",
    "An example of unsupervised learning could use the clustering algorithm. If the dataset consisted of males and females of a particular age and had data for **Height & Weight** - with no labels the clustering algorithm would seperate the data and may provide some distinction bases on the fact that typically males would be taller and heavier than females. We will look at clustering later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is SciKit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple and efficient tool for data mining and daat analysis. It's a library that is built on the NumpPy, SciPy and matplotlib libraries and works well with a lot of other packages such as Pandas for example. It provides the platform to build unsupervised and supervised algorithms that can be used in machine learning such as:\n",
    "\n",
    " - Classification\n",
    " - Regression\n",
    " - Clustering \n",
    " \n",
    "Scikit-Learn is one of the most frequently used machine learning libraries and works very well with Python. It is heavily used in both industry and for academic purposes. The user interface repeatability and amount of training material available means beginner and advanced developers alike can utilise the library for various reasons. The repeatability of use of data between various algorithms without major code changes is a great benefit of this library also. Examples of this will be shown in this Noteook. In python scikit-learn is accessed by importing the sklearn library. Typically this is done by using the following code:\n",
    "\n",
    "`import sklearn as sk`\n",
    " \n",
    "Some of the good features in scikit-learn include:\n",
    "\n",
    "- It comes with a few built in datasets that do not require to download.\n",
    "- Ensemble methods are used for combining the predictions of multiple supervised models.\n",
    "- Feature selection is used for identifying meaningful attributes from which supervised models are created.\n",
    "- Parameter Tuning is used for getting the most out of supervised models.\n",
    "- Manifold Learning that is used for summarizing and depicting complex multi-dimensional data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main steps for creating a Machine learning model wih SciKit-learn\n",
    "\n",
    "<img src=\"../Images for Scikit Learn Noebook/Steps for Machine Learning3.jpg\" width=\"66%\" height=\"66%\" Title=\"Machine Learning\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "---\n",
    "\n",
    "This is the starting point to any machine learning model. Choosing that data that you are looking to analyse. The type of data that you are working with will determine the type of Machine learning algorithm that will be used. Scikit-learn has built in datasets as mentioned above that can be loaded by `from sklearn import datasets` and then using the code `datasets.load_\"name of dataset`. There are also many other sources that have datasets that are ready for analysis. The [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php) or the [KD nuggets](https://www.kdnuggets.com/datasets/index.html) websites have plenty of good sets of data and even provide tutorials on which algorithm would work on each dataset. The datasets from these sites can be loaded by using Pandas and importing a __.csv__ file. The code to do this would be as follows: \n",
    "\n",
    "| |\n",
    "|--------------|\n",
    "| `import pandas as pd` |\n",
    "| `name = pd.read.csv(\"URL/Location of csv\")`  |\n",
    "\n",
    "\n",
    "From this point it is a good idea to get a visualisation of the data using Matplotlib and to check that the data it is complete without null values. When using a Pandas dataframe this can be done easily by using the `head()`, `tail()`, `describe()` and other functions.\n",
    "\n",
    "Using the following code can help to find the information you need about the data. `\"name_of_data\".keys()` will give a list of the attributes available once printed. \n",
    "The `\"name_of_data\".data` attribute will show the data, `name_of_data.target` shows the target values and the `\"name_of_data\".DESCR` will give the description of the data if one is associated with the data.\n",
    "\n",
    "### Pre Processing\n",
    "---\n",
    "\n",
    "Data needs to be _**cleaned/parsed**_ to ensure that data entering the model can be used and makes sense.\n",
    "Sklearn has some built in methods to pre-process the data. The most common methods of this are shown below.\n",
    "Approximately 80% of the time spent on Machine learning model is spent on this step. It is a vitally important step as the model can only perform efficiently when it has __\"clean\"__ data. \n",
    "\n",
    "#### __Standardization__\n",
    "This step is used to rescale data points of one or more of the Feature variables. It rescales the data so the mean and standard deviation values would be 0 and 1 respectively for data that has a bell curve distribution.\n",
    "#### __Normalization__\n",
    "Normalisation is used to ensure no single numeric feature variable is overimpacting the outcome based off the Feature's range of values. It ensures there is a common scale for the features.\n",
    "#### __Binarization__\n",
    "This involves the feature values being set to 0 or 1 depending on the set threshold.\n",
    "Any value greater than the threshold will map to 1 while values less or equal will map to 0. \n",
    "\n",
    "#### __Encoding Categorical Features__\n",
    "This involves taking categorical features/variables as sklearn can't deal with non numeric values.\n",
    "There are two types of categorical values that can be assigned numeric values. These are Ordinal and nominal categories.\n",
    "Ordinal data is data that can be ordered but the distance between the values is not known. This type of data uses the `OrdinalEncoder()`, whereas nominal data has no ordering. Nominal data uses `OneHotEncoder()` to encode the data.\n",
    "\n",
    "#### __Imputing missing values__\n",
    "Filling in the missing data can be done by using the imputing the data or deleting the rows entirely. Imputation normally takes the mean, median or 0 for the missing values.\n",
    "\n",
    "#### __Generating Polynomial Features__\n",
    "This is done by generating a new feature matrix consisting of raising existing features to an exponent. If there was one input feature `A` the polynomial feature would add a new feature column where the values would be calculated by squaring the input feature. \n",
    "\n",
    "### Train and Test Data\n",
    "---\n",
    "When trying to see how the model performs you will need to pass data through and test it. This is done by dividing the data set into a training data set and a test data set. The training data set is used to train the model and the test data set is a way of testing the model with **\"new\"** data. For supervised learning the target output of the test data set will be known to the user making evaluation of accuracy easier. Typically the data set will be split into \n",
    "$$\\frac{2}{3} (Trained Data) \\;\\frac{1}{3} (Test Data)$$\n",
    "The size of the desired test and trained datasets can be easily manipulated by the user. There is also a `random_state` argument that can be set to a constant to allow consistency when testing the model by ensuring the split will remain identical. \n",
    "\n",
    "### Create Model\n",
    "---\n",
    "When choosing the model that the user wants to use to analyse the data there are many factors to take into account. For beginner developers the below __cheatsheet__ is a great starting point if you are unsure with what model to start building. Sometimes it is best to try out various algorithms/compare the accuracy of the models and see which works best for the data set involved.\n",
    "\n",
    "<img src=\"../Images for Scikit Learn Noebook/Machiine learning - choosing models.png\" width=\"60%\" height=\"50%\" Title=\"Machine Learning\"> \n",
    "\n",
    "\n",
    "### Model Fitting\n",
    "---\n",
    "\n",
    "Supervised takes in two arrays (the training dataset and the target values). Unsupervised takes in one array (the training dataset). \n",
    "\n",
    "### Prediction\n",
    "---\n",
    "For the predict step you need to use the algorithm you have chosen to fit the data to and call `fitted_model.predict(X_Test)` with the test data from the test_train_split section.\n",
    "\n",
    "### Evaluate Model\n",
    "---\n",
    "The evaluation of a models are done by using metrics. Some of the more common metrics are shown below. These are imported fromm the sklearn.metrics library using the following code.\n",
    "\n",
    "`from sklearn.metrics import classification_report, confusion_matrix, accuracy_score`\n",
    "\n",
    "\n",
    "|Classification| Regression | Clustering | All|\n",
    "|:-------------:|:-------------:|:-------------:|:-------------:|\n",
    "|Accuracy score |Mean Absolute error|Adjusted Rand Index| Cross Validation\n",
    "|Classification report|Mean Squared Error|V Measure|\n",
    "|Confusion Matrix|$$R^{2}$$|Homogeneity|\n",
    "\n",
    "\n",
    "### Tune Model\n",
    "---\n",
    "\n",
    "https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/\n",
    "Hyperparameters -  Unlike parameters, hyperparameters are specified by the practitioner when configuring the model.\n",
    "The most important hyperparameter for KNN is the number of neighbors (n_neighbors).\n",
    "Random Forest\n",
    "The most important parameter is the number of random features to sample at each split point (max_features).\n",
    "\n",
    "Random Search. Define a search space as a bounded domain of hyperparameter values and randomly sample points in that domain.\n",
    "Grid Search. Define a search space as a grid of hyperparameter values and evaluate every position in the grid\n",
    "\n",
    "\n",
    "Specifically, it provides the RandomizedSearchCV for random search and GridSearchCV for grid search. Both techniques evaluate models for a given hyperparameter vector using cross-validation, hence the “CV” suffix of each class name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Algorithms that will be modelled in this Notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Videos:\n",
    "- https://www.youtube.com/watch?v=pqNCD_5r0IU\n",
    "- https://www.youtube.com/watch?v=0Lt9w-BxKFQ\n",
    "- https://www.youtube.com/watch?v=0B5eIE_1vpU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "* file:///C:/Users/CMHig/Downloads/Machine%20Learning%20research/Introduction%20to%20Machine%20Learning%20with%20Python%20A%20Guide%20for%20Data%20Scientists%20by%20Andreas%20C.%20M%C3%BCller,%20Sarah%20Guido%20(z-lib.org).pdf\n",
    "* file:///C:/Users/CMHig/Downloads/Machine%20Learning%20research/978-1-4842-5373-1.pdf\n",
    "* file:///C:/Users/CMHig/Downloads/Machine%20Learning%20research/Machine%20Learning%20Applications%20Using%20Python%20-%20Cases%20Studies%20from%20Healthcare,%20Retail,%20and%20Finance%20by%20Puneet%20Mathur%20(z-lib.org).pdf\n",
    "* file:///C:/Users/CMHig/Downloads/Machine%20Learning%20research/978-1-4842-3207-1_1.pdf\n",
    "* file:///C:/Users/CMHig/Downloads/Machine%20Learning%20research/Machine%20Learning%20Step-by-Step%20Guide%20To%20Implement%20Machine%20Learning%20Algorithms%20with%20Python%20by%20Rudolph%20Russell%20(z-lib.org).pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms that will be analysed in this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
